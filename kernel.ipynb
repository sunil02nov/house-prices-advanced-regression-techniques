{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor=sns.color_palette()\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn \nfrom scipy import stats\nfrom scipy.stats import skew,norm\nfrom subprocess import check_output\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain=pd.read_csv(\"../input/train.csv\")\ntest=pd.read_csv(\"../input/test.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bbcf6baa6b06fa5153e683866be99f2bd597321"
      },
      "cell_type": "code",
      "source": "##display the first five rows of the train dataset.\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e29f70dfa31f75bc0fd3f36d7ec06bb1ba831f4b"
      },
      "cell_type": "code",
      "source": "test.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86d418620e2a4732f021b7e9056c35c8556099b9"
      },
      "cell_type": "code",
      "source": "\nprint (\"Size of train data after dropping Id: {}\" .format(train.shape))\nprint (\"Size of test data after dropping Id: {}\" .format(test.shape))\ntrain.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a70f561e58310018d9089756ae01a0da68af470a"
      },
      "cell_type": "code",
      "source": "#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "898925b32ff2fdbdba96c237a5b65c9355d18185"
      },
      "cell_type": "code",
      "source": "print (\"Size of train data after dropping Id: {}\" .format(train.shape))\nprint (\"Size of test data after dropping Id: {}\" .format(test.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "756aac6bd1150c22fc161ceb6694389898ebd1d4"
      },
      "cell_type": "code",
      "source": "# Numeric and categorical features in the dataset\ntrain.select_dtypes(include=[np.number]).columns, train.select_dtypes(include=[np.object]).columns\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "34b59f3149446b2f654c994304c314cafb565c8c"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "60c99829d06535b1429818ac3d10170648421682"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Correlation Features</h1>\n\nLet's look at some correlation features between features."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab902c638e6a302404debcff34b022d477c0cd06"
      },
      "cell_type": "code",
      "source": "# Showing the numerical varibales with the highest correlation with 'SalePrice', sorted from highest to lowest\ncorrelation = train.corr()\nprint(correlation['SalePrice'].sort_values(ascending = False))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54a9c00cf468691ddc25b5c3afca7aa68e83356d"
      },
      "cell_type": "code",
      "source": "top_corr_features = correlation.index[abs(correlation['SalePrice'])>0.50]\nplt.figure(figsize=(10,10))\ng=sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49aa42b42fec179f56c5ef78435a173dc3f0c895"
      },
      "cell_type": "code",
      "source": "# Heatmap of correlation of numeric features\nf , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of numeric features',size=15)\nsns.heatmap(correlation,square = True,  vmax=0.8)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1671fea575f46e122a3a80f43c281655911abb1f"
      },
      "cell_type": "markdown",
      "source": "* We observe two white squares (2,2 and 3,3) in the heatmap indicating high correlation. The first group of highly correlated variables is 'TotalBsmtSF' and '1stFlrSF'. The second group is 'GarageYrBlt', 'GarageCars' and 'GarageArea'. This indicates the presence of multicollinearity.\n* The other four white squares (1,1) just indicate an obvious correlation between 'GarageYrBlt' and 'YearBuilt' and between 'TotRmsAbvGrd' and 'GrLivArea'\n* We also observe from the heatmap and the previous correlation list that'GrLivArea', 'TotalBsmtSF', 'OverallQual', 'FullBath', 'TotRmsAbvGrd' and 'YearBuilt' are highly correlated with 'SalePrice'"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54c890f540c37e5b699e08815f0acb0c78973031"
      },
      "cell_type": "code",
      "source": "# Zoomed HeatMap of the most Correlayed variables\nzoomedCorrelation = correlation.loc[['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea'],\n                                    ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea']]\nf , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of numeric features',size=15)\nsns.heatmap(zoomedCorrelation, square = True, linewidths=0.01, vmax=0.8, annot=True,cmap='viridis',\n            linecolor=\"black\", annot_kws = {'size':12})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9aeac95b80ee052c64946d159279ffa30eb76108"
      },
      "cell_type": "markdown",
      "source": "We conclude that :\n\n* 'TotalBsmtSF' and '1stFlrSF' are strongly correlated\n* 'TotRmsAbvGrd' and 'GrLivArea' are strongly correlated\n* 'GarageCars' and 'GarageArea' are strongly correlated\n* 'GarageYrBlt' and 'YearBuilt' are strongly correlated\n* 'TotRmsAbvGrd' and 'GrLivArea' are strongly correlated\n* 'OverallQual', 'GrLivArea' and 'TotRmsAbvGrd' are strongly correlated with 'SalePrice'"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c10d92ccdaae61ca238d86b6b81400cf98c350df"
      },
      "cell_type": "code",
      "source": "sns.barplot(train.OverallQual,train.SalePrice)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6cf046cf64401cf72927b9fe5ebb9298d59cbfd1"
      },
      "cell_type": "code",
      "source": "sns.set()\ncols=['SalePrice','OverallQual','GrLivArea','GarageCars','TotalBsmtSF','FullBath','YearBuilt']\nsns.pairplot(train[cols],size=2.5)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b3e720fc3592ddf179dda18e5c02ba67dd03fd8"
      },
      "cell_type": "code",
      "source": "# Pair plot\nsns.set()\ncols = ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea']\nsns.pairplot(train[cols],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "86ac289e4dbfa284646015cd7aa35ca625020307"
      },
      "cell_type": "markdown",
      "source": "* We observe that 'SalePrice' increases almost quadratically with 'TotalBsmtSF', 'GrLivArea', '1stFlrSF'. So we conclude that the price of the houses increases quadratically with its surface area. We also observe that 'SalePrice' increases exponentially with 'OverallQual'.\n* We also observe from 'GrLivArea'-'1stFlSF' and '1stFlSF'-'TotalBsmSF' that all the points are above the identity fucntion line, which means that the ground living area has the biggest surface of all floors, and that the first floor area is generally bigger than the basement area.\n* We observe the same phenomenon for 'GarageYrBlt'-'YearBuilt'. which makes sense since we start building the garage after building the house, altough there are some exceptions in the data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28de4634ee43219e185427a5a6558937413dd908"
      },
      "cell_type": "code",
      "source": "%%HTML\n<h1>Removing Outliers</h1>",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ef12170d22f4abe7832341c1b6b0152423814061"
      },
      "cell_type": "markdown",
      "source": "From the previous pair plots, we can see that there are outliers for 'TotalBsmtSF', '1stFlrSF' and 'GrLivArea'. Let's use the scatterplot to observe these outliers more precisely"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "508ac4dfd62ed43b2dbbc1a8d376431258933e43"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(7,5))\nplt.scatter(x = train.TotalBsmtSF,y = train.SalePrice)\nplt.title('TotalBsmtSF', size = 15)\nplt.figure(figsize=(7,5))\nplt.scatter(x = train['1stFlrSF'],y = train.SalePrice)\nplt.title('1stFlrSF', size = 15)\nplt.figure(figsize=(7,5))\nplt.scatter(x = train.GrLivArea,y = train.SalePrice)\nplt.title('GrLivArea', size = 15)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39e7d193a347c5b6538df04f61e7425a4b763c11"
      },
      "cell_type": "code",
      "source": "#Removing outliners\ntrain.drop(train[train['TotalBsmtSF']>5000].index,inplace=True)\ntrain.drop(train[train['1stFlrSF']>4000].index,inplace=True)\ntrain.drop(train[(train['GrLivArea']>4000)&(train['SalePrice']<300000)].index,inplace=True)\ntrain.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee6a6e0030f373208726e0dfbda2f7b954b647ad"
      },
      "cell_type": "code",
      "source": "%%HTML\n<h1>Imputation of missing values</h1>",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f4334b8a806f1c01bb64177c4e2a44b512d46a2"
      },
      "cell_type": "markdown",
      "source": "Let's look at the missing valeus in our data. We will be using msno library. Msno provides a small toolset of flexible and easy-to-use missing data visualizations and utilities that allows you to get a quick visual summary of the completeness (or lack thereof) of your dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd8eef78b752392b8c4d523dcc40d4a6bf1f81b4"
      },
      "cell_type": "code",
      "source": "# Visualising missing values of numeric features for sample of 200\nimport missingno as msno # missing data visualizations and utilities\nmsno.matrix(train.select_dtypes(include=[np.number]).sample(200))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d15eec996fc08e026d6d3227e91beaa5168a202"
      },
      "cell_type": "code",
      "source": "# Visualising percentage of missing values of the top 10 numeric variables\ntotal = train.select_dtypes(include=[np.number]).isnull().sum().sort_values(ascending=False)\npercent = (train.select_dtypes(include=[np.number]).isnull().sum()/train.select_dtypes(include=[np.number]).isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Missing Count', 'Missing Percentage'])\nmissing_data.index.name =' Numeric Feature'\nmissing_data.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21e0a9016e2a4277c6e36ed036513318cf580741"
      },
      "cell_type": "markdown",
      "source": "We observe that 'LotFrontage', 'GarageYrBlt' and 'MasVnrArea' are the only one who have missing values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e1c469ea9a2facdc25065426e0888b7a6619c31d"
      },
      "cell_type": "code",
      "source": "# Visualising missing values of categorical features for sample of 200\nmsno.matrix(train.select_dtypes(include=[np.object]).sample(200))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5887187b4b90acee1824a114fb5286c2e7a1863"
      },
      "cell_type": "code",
      "source": "# Visualising percentage of missing values of the top 10 categorical variables\ntotal = train.select_dtypes(include=[np.object]).isnull().sum().sort_values(ascending=False)\npercent = (train.select_dtypes(include=[np.object]).isnull().sum()/train.select_dtypes(include=[np.object]).isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Missing Count', 'Missing Percentage'])\nmissing_data.index.name =' Numeric Feature'\nmissing_data.head(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b22cf204cffc62ba9f580682919c961e0f40c9c"
      },
      "cell_type": "markdown",
      "source": "We observe that 'PoolQC', 'MiscFeature', 'Alley', 'Fence' and 'FireplaceQu' have a significant amount of missing values (at least more than half of observation)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "046c18ac197c1d2c8cb23bb165a5020ce1febbe9"
      },
      "cell_type": "code",
      "source": "# Visualization of nullity by column\nmsno.bar(train.sample(1000))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b77bd21ebe93e28e534cf4eda31078fcd54a848"
      },
      "cell_type": "code",
      "source": "# Nullity correlation heatmap : how strongly the presence or absence of one variable affects the presence of another\nmsno.heatmap(train)\n\n# -1 : if one variable appears the other definitely does not\n# 0 : variables appearing or not appearing have no effect on one another \n# 1 : if one variable appears the other definitely also does",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e2a4751404b6595d1dd020476ae1fb78ca4b2c9"
      },
      "cell_type": "code",
      "source": "# Dendogram for variable completion, reveals trends deeper than the pairwise ones visible in the correlation heatmap\nmsno.dendrogram(train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e503e00fa89ec7d7c3bb60556f6f46646354ab41"
      },
      "cell_type": "markdown",
      "source": "Cluster leaves which linked together at a distance of zero fully predict one another's presence : one variable might always be empty when another is filled, or they might always both be filled or both empty.\n\nFirst of all, let's start by replacing the missing values in both the training and the test set. So we will be combining both datasets into one dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd5e25a83f64280fe7f4c1c50ebb6579f0a13e52"
      },
      "cell_type": "code",
      "source": "# Concatenate the training and test datasets into a single datafram\ndataFull = pd.concat([train,test],ignore_index=True)\n#dataFull.drop('Id',axis = 1,inplace = True)\ndataFull.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a720cf14e5766bcdae764ba7dde82f02d914ae8b"
      },
      "cell_type": "code",
      "source": "# Sum of missing values by feature\nsumMissingValues= dataFull.isnull().sum()\nsumMissingValues[sumMissingValues>0].sort_values(ascending=False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f295e7c02be9032317d044a7b6a95dc3f48c6c7a"
      },
      "cell_type": "markdown",
      "source": "We impute them by proceeding sequentially through features with missing values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c359cef2ff9bb9f367e10aab85fac15b9eb079b5"
      },
      "cell_type": "code",
      "source": "# Impute features with more than five missing values\n\n# Categorical features : replace all with 'None'\nfor col in ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageQual','GarageCond','GarageFinish','GarageType','BsmtExposure','BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1','MasVnrType']:\n    dataFull[col].fillna('None',inplace = True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4248f36027392706fd4bc60ddbb21d00a6f0b637"
      },
      "cell_type": "code",
      "source": "# Categorical features : replace with the mode (most frequently occured value)\nfor col in ['MSZoning','Functional','Utilities','KitchenQual','SaleType','Exterior2nd','Exterior1st','Electrical']:\n    dataFull[col].fillna(dataFull[col].mode()[0],inplace= True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8b0312de88ef74549f44ee9b804de74b04d30ea"
      },
      "cell_type": "code",
      "source": "# Numeric features : replace with 0\nfor col in ['BsmtFullBath','BsmtHalfBath','BsmtUnfSF','TotalBsmtSF','GarageCars','BsmtFinSF2','BsmtFinSF1','GarageArea']:\n    dataFull[col].fillna(0,inplace= True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e41a97d34c12fe61fb6ef4052b8c0c07513cd852"
      },
      "cell_type": "code",
      "source": "dataFull['MasVnrArea'].fillna(dataFull['MasVnrArea'].mean(), inplace=True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3f22049ad0cf86a7266dcba94b0cfbac6db1e01d"
      },
      "cell_type": "markdown",
      "source": "Based on the previous correlation heatmap, 'GarageYrBlt' is highly correlated with 'YearBuilt', so let's replace the missing values by medians of 'YearBuilt'. To do that, we need to cut 'YearBuilt' into sections since it is a numeric variable"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f486e3c4f07be1c6b27afcf1427b6921f87efc8"
      },
      "cell_type": "code",
      "source": "# Cut 'YearBuilt' into 10 parts\ndataFull['YearBuiltCut'] = pd.qcut(dataFull.YearBuilt,10)\n# Impute the missing values of 'GarageYrBlt' based on the median of 'YearBuilt' \ndataFull['GarageYrBlt']= dataFull.groupby(['YearBuiltCut'])['GarageYrBlt'].transform(lambda x : x.fillna(x.median()))\n# convert the values to integers\ndataFull['GarageYrBlt'] = dataFull['GarageYrBlt'].astype(int)\n# Drop 'YearBuiltCut' column\ndataFull.drop('YearBuiltCut',axis=1,inplace=True)\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd217c9184777941152b7a987e69144df6dda669"
      },
      "cell_type": "code",
      "source": "# Cut 'LotArea' into 10 parts\ndataFull['LotAreaCut'] = pd.qcut(dataFull.LotArea,10)\n# Impute the missing values of 'LotFrontage' based on the median of 'LotArea' and 'Neighborhood'\ndataFull['LotFrontage']= dataFull.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\ndataFull['LotFrontage']= dataFull.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\n# Drop 'LotAreaCut' column\ndataFull.drop('LotAreaCut',axis=1,inplace=True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9579e127e0b9811559c2992ac65bc9957ab4fb9f"
      },
      "cell_type": "markdown",
      "source": "The only missing values that are left are within SalePrice, which is exactly the number of lignes in the test data (the values that we need to predict)"
    },
    {
      "metadata": {
        "_uuid": "c0d5c36cfd50b46b6d33371f22b57b88a13598c4"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Correcting Features</h1>"
    },
    {
      "metadata": {
        "_uuid": "db0778b3c7daf15faa9dffbecd2e86a3183ab39e"
      },
      "cell_type": "markdown",
      "source": "If we take a look at the numeric variables, we see that some of them obviously don't make a sense being numerical like year related features. Let's take a closer look at each one of them in the data description file and see which ones need to be converted to categorical type."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c1772995777e20c5411e4655dbcb6a6ddf50e23"
      },
      "cell_type": "code",
      "source": "dataFull.select_dtypes(include=[np.number]).columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92cbd85be6f5f08aceeb8b9d784d4c98d67a407b"
      },
      "cell_type": "code",
      "source": "# Converting numeric features to categorical features\nstrCols = ['YrSold','YearRemodAdd','YearBuilt','MoSold','MSSubClass','GarageYrBlt']\nfor i in strCols:\n    dataFull[i]=dataFull[i].astype(str)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8502ffd7821ef91fa0f4f62e5a0c8bda39757626"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Adding Features</h1>"
    },
    {
      "metadata": {
        "_uuid": "889f922dbdb7f0bae182d1239de4bd153248bf9e"
      },
      "cell_type": "markdown",
      "source": "First, we will map some categorical variable that represent some sort of rating to an integer score"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdb0c3789dc49a6442318855010a789d3b703950"
      },
      "cell_type": "code",
      "source": "dataFull.select_dtypes(include=[np.object]).columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1381413c5c5699fb22030c08bf36b6ccf7732936"
      },
      "cell_type": "code",
      "source": "dataFull[\"oExterQual\"] = dataFull.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndataFull[\"oBsmtQual\"] = dataFull.BsmtQual.map({'None':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndataFull[\"oBsmtExposure\"] = dataFull.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\ndataFull[\"oHeatingQC\"] = dataFull.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndataFull[\"oKitchenQual\"] = dataFull.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndataFull[\"oFireplaceQu\"] = dataFull.FireplaceQu.map({'None':1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\ndataFull[\"oGarageFinish\"] = dataFull.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\ndataFull[\"oPavedDrive\"] = dataFull.PavedDrive.map({'N':1, 'P':2, 'Y':3})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "28a779377aaca9eb0b11bd4832d29f7c28fb8ca8"
      },
      "cell_type": "markdown",
      "source": "Next, we will add up some numeric features with each other to create new features that make sense\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9976f858c020dcfe98a28c46662811abead7b70"
      },
      "cell_type": "code",
      "source": "dataFull.select_dtypes(include=[np.number]).columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c21b0e46d001a2bb207ea411b0962d26879fd5a3"
      },
      "cell_type": "code",
      "source": "dataFull['HouseSF'] = dataFull['1stFlrSF'] + dataFull['2ndFlrSF'] + dataFull['TotalBsmtSF']\ndataFull['PorchSF'] = dataFull['3SsnPorch'] + dataFull['EnclosedPorch'] + dataFull['OpenPorchSF'] + dataFull['ScreenPorch']\ndataFull['TotalSF'] = dataFull['HouseSF'] + dataFull['PorchSF'] + dataFull['GarageArea']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8c0fcdbbef5efb3b5e2871aabb4b1dd0860f09db"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Skewness and Kurtosis</h1>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1141c5bb2151fea470c0f4a34d20a4204082cb43"
      },
      "cell_type": "code",
      "source": "# Estimate Skewness and Kurtosis of the data\ntrain.skew(), train.kurt()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41a33751114a22297c80ad904b9d6d8d86f5f24b"
      },
      "cell_type": "code",
      "source": "# Plot the Skewness of the data\nsns.distplot(train.skew(),axlabel ='Skewness')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "563a5c2b4d498ce7a7687e29f712901761bfea64"
      },
      "cell_type": "code",
      "source": "# Plot the Kurtosis of the data\nsns.distplot(train.kurt(),axlabel ='Kurtosis')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "07296956e22fb18076388d55cfddc40f42a8b88f"
      },
      "cell_type": "markdown",
      "source": "There isn't much kurtosis in the data columns, but Skewness is very present, meaning that distribution is not symetrical"
    },
    {
      "metadata": {
        "_uuid": "8268d643e75ae3af3f173cc5e6c3414ee7e8e438"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Label Encoding</h1>"
    },
    {
      "metadata": {
        "_uuid": "df9191fcf239daaa8d9cdf98896da8ae1d89e1c3"
      },
      "cell_type": "markdown",
      "source": "For this section we will use Pipelines which are a way to streamline a lot of the routine processes. It provides a way to take code, fit it to the training data, apply it to the test data without having to copy and paste everything. We will create three classes : first for label encoding, second for skewness, and third for one hot label encoding."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47e7f99352f35212d6d89296336dd675dffaba61"
      },
      "cell_type": "code",
      "source": "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import LabelEncoder, Imputer\nfrom scipy.stats import skew\n\n# Label encoding class\nclass labenc(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        label = LabelEncoder()\n        X['YrSold']=label.fit_transform(X['YrSold'])\n        X['YearRemodAdd']=label.fit_transform(X['YearRemodAdd'])\n        X['YearBuilt']=label.fit_transform(X['YearBuilt'])\n        X['MoSold']=label.fit_transform(X['MoSold'])\n        X['GarageYrBlt']=label.fit_transform(X['GarageYrBlt'])\n        return X\n    \n# Skewness transform class\nclass skewness(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        skewness = X.select_dtypes(include=[np.number]).apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= 1].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        return X\n\n# One hot encoding class\nclass onehotenc(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        X = pd.get_dummies(X)\n        return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c1e79ecd92582585d7168fff8fbd0f9aa981a14"
      },
      "cell_type": "code",
      "source": "# Creating a copy of the full dataset\ndataFullCopy = dataFull.copy()\n\n# Creating a new fata with aplied transformations using sklearn Pipeline\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([('labenc',labenc()),('skewness',skewness()),('onehotenc',onehotenc())])\ndataPipeline = pipeline.fit_transform(dataFullCopy)\ndataFull.shape, dataPipeline.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "333475bd40de40a0b8d8262aecbf84a98c473f9c"
      },
      "cell_type": "markdown",
      "source": "We can see now that the number of features increases from 88 to 328"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbd47aa2acb09d9c65543a739d731f5224e5b151"
      },
      "cell_type": "code",
      "source": "dataPipeline.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "84da45216ecb22032fb7e327fd353543a0ad2c57"
      },
      "cell_type": "markdown",
      "source": "Now we split the data to training and testing datasets"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08073f7ff44948434866118623d7826318b208d0"
      },
      "cell_type": "code",
      "source": "\nX_train = dataPipeline[:train.shape[0]]\ny_train = X_train['SalePrice']\nX_train.drop(columns = 'SalePrice', inplace=True)\nX_test = dataPipeline[train.shape[0]:]\nX_test.drop(columns = 'SalePrice', inplace=True)\nX_train.shape, y_train.shape, X_test.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f7cee77f5e5b0ea87cd8c0821992c9d599b5b644"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Transformation and Scaling</h1>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "326f47425a7077abe5dda7be3672fc1f2467e592"
      },
      "cell_type": "code",
      "source": "# SalesPrices plot with three different fitted distributions\nimport scipy.stats as st # statistics\nplt.figure(2); plt.title('Normal')\nsns.distplot(y_train, kde=False, fit=st.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y_train, kde=False, fit=st.lognorm)\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y_train, kde=False, fit=st.johnsonsu)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aff4cf79b5659cb1ea4900ec3350269dd2593700"
      },
      "cell_type": "markdown",
      "source": "Normal distribution doesn't fit, so SalePrice need to be transformed before creating the model. Best fit is unbounded Johnson distribution, altough log normal distribution also fits well"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5aebe15b586b3c99e25a59607a45bb4ee2bbcd55"
      },
      "cell_type": "code",
      "source": "# transforming 'SalePrice' into normal distribution\ny_train_transformed = np.log(y_train)\ny_train_transformed.skew(), y_train_transformed.kurt()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ac8488553596bf6acbb06f6a1233fef2db3a373"
      },
      "cell_type": "code",
      "source": "# plotting 'SalePrice' before and after the transformation\nplt.figure(1); plt.title('Before transformation')\nsns.distplot(y_train)\nplt.figure(2); plt.title('After transformation')\nsns.distplot(y_train_transformed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29e7b1f5d100f9d12e36dedf4fae87369b72c925"
      },
      "cell_type": "code",
      "source": "# Using Robust Scaler to transform X_train\nfrom sklearn.preprocessing import RobustScaler\nrobust_scaler = RobustScaler()\nX_train_scaled = robust_scaler.fit(X_train).transform(X_train)\nX_test_scaled = robust_scaler.transform(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "521d5962e73276a14caae8606b4f42ada8a596f0"
      },
      "cell_type": "code",
      "source": "# Shape of final data we will be working on\nX_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a97f4e9a699a8fab681ac1165bd88c00ef4272ad"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Feature Selection</h1>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "913b1d6a0f472e49e823fee5c6461cb3a0082582"
      },
      "cell_type": "markdown",
      "source": "We will use lasso regression (l1 regularization method). Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. We can also use it to find the most important features in our dataset."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d87e597703ff37fdb3495770a772bcfa127dfe7"
      },
      "cell_type": "code",
      "source": "# Display features by their importance (lasso regression coefficient)\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 0.001)\nlasso.fit(X_train_scaled,y_train_transformed)\ny_pred_lasso = lasso.predict(X_test_scaled)\nlassoCoeff = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=dataPipeline.drop(columns = 'SalePrice').columns)\nlassoCoeff.sort_values(\"Feature Importance\",ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0965f950a9398cffb1732aa1548c39620e7b1667"
      },
      "cell_type": "code",
      "source": "# Plot features by importance (feature coefficient in the model)\nlassoCoeff[lassoCoeff[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(20,35))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9084f3f83a2b6c346efabbaa7c117fa02c40f23e"
      },
      "cell_type": "markdown",
      "source": "What's intersting here is that two of the variables that we have created 'HouseSF' and 'PorchSF' perform actually bad compared to their components. But when we sum all the surfaces as in 'TotalSF', which is just a combination of features that are significantly unimportant in this model, we suddently obtain the most important feature in the dataset."
    },
    {
      "metadata": {
        "_uuid": "139060404a643d819947f3e38e09d37f5e099078"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Principal Component Analysis</h1>"
    },
    {
      "metadata": {
        "_uuid": "67348e829b06c0c80971499b2c02dc960acb7e9a"
      },
      "cell_type": "markdown",
      "source": "PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ca6b255d745cb5e32a1eaa8f4a0dc355980025f"
      },
      "cell_type": "code",
      "source": "from sklearn.decomposition import PCA\n# Concatenate the training and test datasets into a single datafram\ndataFull2 = np.concatenate([X_train_scaled,X_test_scaled])\n# Choose the number of principle components such that 95% of the variance is retained\npca = PCA(0.95)\ndataFull2 = pca.fit_transform(dataFull2)\nvarPCA = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n# Principal Component Analysis of data\nprint(varPCA)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5129cad8b8642c4cc03c8bcf75ad4521c4c88cff"
      },
      "cell_type": "code",
      "source": "# Principal Component Analysis plot of the data\nplt.figure(figsize=(16,12))\nplt.bar(x=range(1,len(varPCA)+1), height = varPCA)\nplt.ylabel(\"Explained Variance (%)\", size = 15)\nplt.xlabel(\"Principle Components\", size = 15)\nplt.title(\"Principle Component Analysis Plot : Training Data\", size = 15)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d90d978b431c962313895895c01224ed987e940e"
      },
      "cell_type": "code",
      "source": "# Shape of final data we will be working on\nX_train_scaled = dataFull2[:train.shape[0]]\nX_test_scaled = dataFull2[train.shape[0]:]\nX_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d10e97e4eba68a9ebcc1265c182bb407f055bd4a"
      },
      "cell_type": "markdown",
      "source": "We see that now we have 87 features instead of the 327 features that we had before using PCA."
    },
    {
      "metadata": {
        "_uuid": "0f5677835d4355557866959c8d4c0bf051659993"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Testing Different Models</h1>"
    },
    {
      "metadata": {
        "_uuid": "a2f0dcb9df5e733bd9944d5351c456b5fc937301"
      },
      "cell_type": "markdown",
      "source": "Now that we have finished preparing our data, it's time to test different models to see which one performs the best. The models we will be testing are :\n\n* Linear regression\n* Support vector regression\n* Stochastic gradient descent\n* Gradient boosting tree\n* Random forest\n* Lasso regression\n* Ridge regression\n* Elastic net regularization\n* Extra trees regression\n* Kernel Ridge Regression\n* Gradient Boosting Regression\n* XGBoost \n* LightGBM"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d5a40b8e4cb32ae861d0d9dc541575904d952bd"
      },
      "cell_type": "code",
      "source": "# importing the models\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, Lasso, SGDRegressor, Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.svm import LinearSVR,SVR\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nwarnings.filterwarnings('ignore')\n# creating the models\nmodels = [\n             LinearRegression(),\n             SVR(),\n             SGDRegressor(),\n             SGDRegressor(max_iter=1000, tol = 1e-3),\n             GradientBoostingRegressor(),\n             RandomForestRegressor(),\n             Lasso(),\n             Lasso(alpha=0.01,max_iter=10000),\n             Ridge(),\n             BayesianRidge(),\n             KernelRidge(),\n             KernelRidge(alpha=0.6,kernel='polynomial',degree = 2,coef0=2.5),\n             ElasticNet(),\n             ElasticNet(alpha = 0.001,max_iter=10000),    \n             ExtraTreesRegressor(),\n             ]\n\nnames = ['Linear regression','Support vector regression','Stochastic gradient descent','Stochastic gradient descent 2','Gradient boosting tree','Random forest','Lasso regression','Lasso regression 2','Ridge regression','Bayesian ridge regression','Kernel ridge regression','Kernel ridge regression 2','Elastic net regularization','Elastic net regularization 2','Extra trees regression']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cdf6db701872f603881e96038001db4f69788e3"
      },
      "cell_type": "code",
      "source": "# Define a root mean square error function\ndef rmse(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model,X,y,scoring=\"neg_mean_squared_error\",cv=5))\n    return rmse",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "678ccc1a86deb7d0dad31b08f4674cd3cc503271"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold,cross_val_score\nwarnings.filterwarnings('ignore')\n\n# Perform 5-folds cross-calidation to evaluate the models \nfor model, name in zip(models, names):\n    # Root mean square error\n    score = rmse(model,X_train_scaled,y_train_transformed)\n    print(\"- {} : mean : {:.6f}, std : {:4f}\".format(name, score.mean(),score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "13cdf77f7848afb4dcd8be026e9982549fcec722"
      },
      "cell_type": "markdown",
      "source": "Surprisingly, the Random forest and Extra trees regression models are the ones who performed the worst, and the linear regression model performed actually pretty good relative to the other models. By compiling the above code several times and observing the different scores each time, we can classify the models by accuracy :\n\n1st : Kernel ridge regression\n2nd : Elastic net regularization and Bayesian ridge regression\n3rd : Ridge regression and Linear regression\n4rth : Support vector regression\n5th : Gradient boosting tree\n6th : Stochastic gradient and Lasso regression\n7th : Random forest and Extra trees regression\nI think we got a good score in Elastic net regularization, Lasso regression and Stochastic gradient descent because we chose some good parameters. We can see that their score above is very bad when not specifing parameter values. So if we really want to know to best model, we need to choose optimal parameters for all the models, and tha's what we will do in the next section."
    },
    {
      "metadata": {
        "_uuid": "39d0ccd1b7a38085249ad5eca2d7a5d29490e409"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Hyper-parameter Tuning</h1>"
    },
    {
      "metadata": {
        "_uuid": "cfba14668aab450f238c117d99a41f4e54cdde93"
      },
      "cell_type": "markdown",
      "source": "For choosing the most optimal hyper-parameters, we will perform gird search. the class GridSearchCV exhaustively considers all parameter combinations and generates candidates from a grid of parameter values specified with the param_grid parameter. Since we will use the same procedure for all models, we will start by creating a function which takes specified parameter values as entry."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cedb4804c3a2df21e88f6e46c8835a57834fd7f1"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\n\nclass gridSearch():\n    def __init__(self,model):\n        self.model = model\n    def grid_get(self,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5,scoring='neg_mean_squared_error')\n        grid_search.fit(X_train_scaled,y_train_transformed)\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])\n        print('\\nBest parameters : {}, best score : {}'.format(grid_search.best_params_,np.sqrt(-grid_search.best_score_)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e799a4e4e56a72c768602bd01513d8fd41c77fb"
      },
      "cell_type": "markdown",
      "source": "1. Kernel ridge regression\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c43007522e30d4bab9f53c68f6459a8ae4625f52"
      },
      "cell_type": "code",
      "source": "gridSearch(KernelRidge()).grid_get(\n        {'alpha':[3.5,4,4.5,5,5.5,6,6.5], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[1,1.5,2,2.5,3,3.5]})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0a6ecbe573f86c0fd1853dc3e180f5f3d918f3eb"
      },
      "cell_type": "markdown",
      "source": "1. Elastic net regularization"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f18f1e37f5900b960c2afeeab07aa45497ed22c"
      },
      "cell_type": "code",
      "source": "gridSearch(ElasticNet()).grid_get(\n        {'alpha':[0.006,0.0065,0.007,0.0075,0.008],'l1_ratio':[0.070,0.075,0.080,0.085,0.09,0.095],'max_iter':[10000]})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2737d984b392e9f51a084ecec8b7e28041f0d473"
      },
      "cell_type": "markdown",
      "source": "Ridge regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "deb76b086a25ab83c99e58653076277bc7f0ff11"
      },
      "cell_type": "code",
      "source": "gridSearch(Ridge()).grid_get(\n        {'alpha':[10,20,25,30,35,40,45,50,55,57,60,65,70,75,80,100],'max_iter':[10000]})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b5c999a3fb97d28969225a86a3a1476949eaaad"
      },
      "cell_type": "markdown",
      "source": "Support vector regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a046afe988a42883749d6c410e392d18e13e1e80"
      },
      "cell_type": "code",
      "source": "gridSearch(SVR()).grid_get(\n        {'C':[13,15,17,19,21],'kernel':[\"rbf\"],\"gamma\":[0.0005,0.001,0.002,0.01],\"epsilon\":[0.01,0.02,0.03,0.1]})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eae5d7d5b10890edd5650b7b8bcd51f06f286af9"
      },
      "cell_type": "markdown",
      "source": "Lasso regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06197f0465e4acfaa8cae3aaee5eec8c15a64a7c"
      },
      "cell_type": "code",
      "source": "gridSearch(Lasso()).grid_get(\n       {'alpha':[0.01,0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009],'max_iter':[10000]})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd86b6a1efdccf5118e90c8ec3839f90bdbb5ebd"
      },
      "cell_type": "markdown",
      "source": "We see that the models perform almost the same way with a score of 0.116. Let's define these models with the their respective best hyper-parameters."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a67d4445d056c8787676381c59ed2e372f08011"
      },
      "cell_type": "code",
      "source": "lasso = Lasso(alpha= 0.0006, max_iter= 10000)\nridge = Ridge(alpha=35, max_iter= 10000)\nsvr = SVR(C = 13, epsilon= 0.03, gamma = 0.001, kernel = 'rbf')\nker = KernelRidge(alpha=6.5 ,kernel='polynomial', degree=3 , coef0=2.5)\nela = ElasticNet(alpha=0.007,l1_ratio=0.07,max_iter=10000)\nbay = BayesianRidge()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d34605794c18a928e6e3f16e66505c18c14b9759"
      },
      "cell_type": "code",
      "source": "%%HTML\n<h1>Combining Models</h1>",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d07d6d01db368a121918aea97c1c1db8075a9599"
      },
      "cell_type": "markdown",
      "source": "In order to further improve our model accuracy. We will use an ensemble method. I chose to use stacking. Stacked generalization is an ensemble method where the models are combined using another machine learning algorithm. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combiner machine learning algorithm."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d7f6fbffdbb7b399ed51c1f74b46d70849be495"
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor\n# Creating the stacking function\nclass stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,mod,meta_model):\n        self.mod = mod\n        self.meta_model = meta_model\n        kff = KFold(n_splits=5, random_state=42, shuffle=True)\n        self.kf = kff\n        \n    def fit(self,X,y):\n        self.saved_model = [list() for i in self.mod]\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        \n        for i,model in enumerate(self.mod):\n            for train_index, val_index in self.kf.split(X,y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index,i] = renew_model.predict(X[val_index])\n        \n        self.meta_model.fit(oof_train,y)\n        return self\n    \n    def predict(self,X):\n        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)\n    \n    def get_oof(self,X,y,test_X):\n        oof = np.zeros((X.shape[0],len(self.mod)))\n        test_single = np.zeros((test_X.shape[0],5))\n        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n        for i,model in enumerate(self.mod):\n            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n                clone_model = clone(model)\n                clone_model.fit(X[train_index],y[train_index])\n                oof[val_index,i] = clone_model.predict(X[val_index])\n                test_single[:,j] = clone_model.predict(test_X)\n            test_mean[:,i] = test_single.mean(axis=1)\n        return oof, test_mean",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9d85e01cfdb330fc53cc3686054d10f3cc46235"
      },
      "cell_type": "code",
      "source": "# Impute the training dataset\nX_scaled_imputed = Imputer().fit_transform(X_train_scaled)\ny_log_imputed = Imputer().fit_transform(y_train_transformed.values.reshape(-1,1)).ravel()\n\nX_scaled_imputed.shape,y_log_imputed.shape,X_test_scaled.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4884834afe9564060be5981d479d244340d7b529"
      },
      "cell_type": "code",
      "source": "# Calculating the score\nstack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)\nscore = rmse(stack_model,X_scaled_imputed,y_log_imputed)\nprint(score.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fd028cb9c444fd50ab0810500271bbcf526a27de"
      },
      "cell_type": "markdown",
      "source": "We obtain a score of 0.113, which is slightly better than the average score of the other models"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4abedbcc435a3b3593495271b1753edb2f3fa392"
      },
      "cell_type": "code",
      "source": "# Combining the extracted features generated from stacking whith original features\nX_train_stack,X_test_stack = stack_model.get_oof(X_scaled_imputed,y_log_imputed,X_test_scaled)\nX_train_add = np.hstack((X_scaled_imputed,X_train_stack))\nX_test_add = np.hstack((X_test_scaled,X_test_stack))\nX_train_add.shape,X_test_add.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "564bd7f9582ac368fbe9213640208a50aa341676"
      },
      "cell_type": "code",
      "source": "# Calculate the final score of the model\nscore = rmse(stack_model,X_train_add,y_log_imputed)\nprint(score.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "829e8b6a6d530b8e3491f9ad0bd4c8bd3a65b2e7"
      },
      "cell_type": "markdown",
      "source": "The last score we obtain is 0.1074, which is quite good"
    },
    {
      "metadata": {
        "_uuid": "507b6e4cbfdee822696080b27fc5e61e9cc0db8a"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Making Predictions</h1>\n\nNow it's time to make predictions and store them in a csv file with corresponding Ids. after we make prediction we need to transform them to their original shape with exponential function"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06832be8205c2e59b6378ba52042d9b8edda201b"
      },
      "cell_type": "code",
      "source": "# Fit the model to the dataset generated with stacking\nstack_model.fit(X_train_add,y_log_imputed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "520e81dfee07b1df952dd2ddeb26ef848c39c0e8"
      },
      "cell_type": "code",
      "source": "# Making prediction on the test set generated by stacking\npredicted_prices = np.exp(stack_model.predict(X_test_add))\n# Prepare the csv file\n#my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n#my_submission.to_csv('submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e610c5309afccbacb0400fa4f1753f8e1be32b5"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Other Way to Predication</h1>\nDefine a cross validation strategy"
    },
    {
      "metadata": {
        "_uuid": "d7353765fbb161419fc857bc722cde35a64cb14d"
      },
      "cell_type": "markdown",
      "source": "We use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8bc1e66653f49f692e5629732b44d6f52233e5e"
      },
      "cell_type": "code",
      "source": "#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train_scaled,y_train_transformed, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5de3008739f70ca1c172dd53d0fd7d025a4a2002"
      },
      "cell_type": "code",
      "source": "import xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2d27941abd6e87a4547d7ab1cd78327e026c4851"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Base models</h1>"
    },
    {
      "metadata": {
        "_uuid": "a8721272d5f898cc233b814db200335404e7a541"
      },
      "cell_type": "markdown",
      "source": "**LASSO Regression :**\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline[](http://)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5dc4e6b9992ab915e838b3fe3ed423d4c501c39"
      },
      "cell_type": "code",
      "source": "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3fdce62ce997c0c7b781bbc626f2cf4eff33fe98"
      },
      "cell_type": "markdown",
      "source": "**Elastic Net Regression :**\nagain made robust to outliers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "017b08609fcb0a5f4fcccff24f72c19041ac6067"
      },
      "cell_type": "code",
      "source": "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "85efad7f29a141f21a06a3b569795812b04dcb23"
      },
      "cell_type": "markdown",
      "source": "**Kernel Ridge Regression :**\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a0238d9eb32ce6fe6fc5c585dc1704955d9bfde3"
      },
      "cell_type": "code",
      "source": "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a2150c9d0c00293febe28aa85d7dff421312844"
      },
      "cell_type": "markdown",
      "source": "**Gradient Boosting Regression :**\nWith huber loss that makes it robust to outliers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d370b6eb9c56cac9fb69c0a5138d64bf630fefba"
      },
      "cell_type": "code",
      "source": "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1706dcee41d5849bc729fb7197249f0d2b281822"
      },
      "cell_type": "markdown",
      "source": "**XGBoost :**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca509f082a4d79e5bc4aa046a64e2497d799f399"
      },
      "cell_type": "code",
      "source": "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b8370cac1d48a541524df27c5d6c47401cd85b70"
      },
      "cell_type": "markdown",
      "source": "**LightGBM :**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed1c9d8bdf5961961aca649dc47184a5260cb29d"
      },
      "cell_type": "code",
      "source": "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "df501c73377e525a7149fcea933c89ff49160656"
      },
      "cell_type": "markdown",
      "source": "**Base models scores¶**"
    },
    {
      "metadata": {
        "_uuid": "a15ad688ba7cadf1d0209ebb392955c62175361a"
      },
      "cell_type": "markdown",
      "source": "Let's see how these base models perform on the data by evaluating the cross-validation rmsle error"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "547025c77ed8d16f280a0577af9459ee88a4e7c1"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba469fd9bf18805784b6c395e50b45c3825167aa"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1904216edab0a5489ded00eceb7e86fdf2a3abc7"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "943ab12372061684e446d54b1b1baaa349aae8f8"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a75a5871a9667443644a4555ec5ec348a4b4c3d9"
      },
      "cell_type": "markdown",
      "source": "**XGB**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b6a5e999fde223422df0010bcda523f04f98996"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aadc1997b6d0111ea5212366f8d4a61ce62fd868"
      },
      "cell_type": "markdown",
      "source": "**LGBM**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "873c118c80ab813704b4f4b532c785cd8f5bc6ca"
      },
      "cell_type": "code",
      "source": "score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "615fa6fa42f17a7e86e9defef452045e39e69d11"
      },
      "cell_type": "markdown",
      "source": "%%HTML\n<h1>Stacking models</h1>"
    },
    {
      "metadata": {
        "_uuid": "88203e0a0007501548d9d6a146d46ff95b6e15be"
      },
      "cell_type": "markdown",
      "source": "**Simplest Stacking approach : Averaging base models**"
    },
    {
      "metadata": {
        "_uuid": "231a0cfef7b9ff7a5e639da6a0d783d30aa39e0a"
      },
      "cell_type": "markdown",
      "source": "We begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)"
    },
    {
      "metadata": {
        "_uuid": "dbd3d98b02132c2664527c23e3a1df2c96016540"
      },
      "cell_type": "markdown",
      "source": "**Averaged base models class**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e75b8ca71cac121a5d40c2b13fcd13859932d70"
      },
      "cell_type": "code",
      "source": "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a3e76abd3eaa34e454267241f34c24297fd9e5c3"
      },
      "cell_type": "markdown",
      "source": "**Averaged base models score**"
    },
    {
      "metadata": {
        "_uuid": "a6c29efa433d7b6675953667796c303adf38670f"
      },
      "cell_type": "markdown",
      "source": "The last time I submitted, I was the 859th (top 20%) with a score of 0.11934"
    },
    {
      "metadata": {
        "_uuid": "85527eae9f08d5efd8a46802eda7ffa730374b94"
      },
      "cell_type": "markdown",
      "source": "We just average four models here** ENet, GBoost, KRR and lasso**. Of course we could easily add more models in the mix."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7bcf7d7ca5ca7b9e60c12e23b9f88710200cf309"
      },
      "cell_type": "code",
      "source": "averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5cdb6f3dd2bc1e5bf5517493a1e88b89cbc83e1e"
      },
      "cell_type": "markdown",
      "source": "Wow ! It seems even the simplest stacking approach really improve the score . This encourages us to go further and explore a less simple stacking approch."
    },
    {
      "metadata": {
        "_uuid": "b529641b08d1797cbe6d474ef8e0ec90ba97705e"
      },
      "cell_type": "markdown",
      "source": "**Less simple Stacking : Adding a Meta-model¶**"
    },
    {
      "metadata": {
        "_uuid": "2983bcff82045653c190a5cdeda65e02a4a71849"
      },
      "cell_type": "markdown",
      "source": "In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\n* The procedure, for the training part, may be described as follows:\n\n* Split the total training set into two disjoint sets (here train and .holdout )\n \n* Train several base models on the first part (train)\n \n* Test these base models on the second part (holdout)\n\nUse the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model."
    },
    {
      "metadata": {
        "_uuid": "dac2f848e1c4599add322b97fb171d6cc6ef6a87"
      },
      "cell_type": "markdown",
      "source": "**Stacking averaged Models Class**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d6aa76d9c00d502c065126ec6f5822b5706d472"
      },
      "cell_type": "code",
      "source": "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f084ab8c6ca5dab0d58e827a833a2e8f5c650f2e"
      },
      "cell_type": "markdown",
      "source": "**Stacking Averaged models Score**"
    },
    {
      "metadata": {
        "_uuid": "5670a1e100a24c86444d9d7ac870cf95acff613c"
      },
      "cell_type": "markdown",
      "source": "To make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d2538a87d07338c5c4aaaa6a1b18faa7002405c"
      },
      "cell_type": "code",
      "source": "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\n#score = rmsle_cv(stacked_averaged_models)\n#print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'StackingAveragedModels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d9fff82175eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                  meta_model = lasso)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#score = rmsle_cv(stacked_averaged_models)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'StackingAveragedModels' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8f29b57daa01a9165c43e1ca8c893ed7b666adba"
      },
      "cell_type": "markdown",
      "source": "**Ensembling StackedRegressor, XGBoost and LightGBM¶**"
    },
    {
      "metadata": {
        "_uuid": "d60b6eb605bd8a59363a26e027e9d83be2fb2666"
      },
      "cell_type": "markdown",
      "source": "We add **XGBoost** and **LightGBM** to the StackedRegressor defined previously.\nWe first define a rmsle evaluation function"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61f3002cebd6c32ac3799046dca35db6cb1b18cf"
      },
      "cell_type": "code",
      "source": "def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e6175a89361baffe45d8baff61196c1f2095756d"
      },
      "cell_type": "markdown",
      "source": "**Final Training and Prediction**"
    },
    {
      "metadata": {
        "_uuid": "c5df27a4bb79310d52f18f0dff2c9df920fe0a48"
      },
      "cell_type": "markdown",
      "source": "**StackedRegressor:**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c2e8774277ea0a731418dbf82890dca577b383b"
      },
      "cell_type": "code",
      "source": "\n\nstacked_averaged_models.fit(X_train_scaled,y_train_transformed)\nstacked_train_pred = stacked_averaged_models.predict(X_train_scaled)\n#stacked_pred = np.expm1(stacked_averaged_models.predict(y_train_transformed))\n#print(rmsle(y_train_transformed, stacked_train_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e66ef7e0c5ee835c39e084236b5057b32a9c59b"
      },
      "cell_type": "markdown",
      "source": "We get again a better score by adding a meta learner"
    },
    {
      "metadata": {
        "_uuid": "1a69f116dd5dd32adf3efc8b0bac67124ea0dcd5"
      },
      "cell_type": "markdown",
      "source": "**Ensembling StackedRegressor, XGBoost and LightGBM¶**"
    },
    {
      "metadata": {
        "_uuid": "0ec90c91aeaa40987cee2c0ceacee2c7e2d1db80"
      },
      "cell_type": "markdown",
      "source": "We add **XGBoost** and **LightGBM** to the StackedRegressor defined previously."
    },
    {
      "metadata": {
        "_uuid": "99f3ec47810d179f074aa1f58b65b87f2ac554a6"
      },
      "cell_type": "markdown",
      "source": "We first define a rmsle evaluation function"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fe36f0f95db53727e8cfd497ee9c6e4cd233e33"
      },
      "cell_type": "code",
      "source": "def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd35516cc0d718e887476cceacd97480e02c4498"
      },
      "cell_type": "markdown",
      "source": "**Final Training and Prediction**"
    },
    {
      "metadata": {
        "_uuid": "6c2a432cfc3e7832ef8e254433250c4beb8fa2fa"
      },
      "cell_type": "markdown",
      "source": "**StackedRegressor:**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8c9999cd4c6a3d920f1f20923f533d2360b6eca"
      },
      "cell_type": "code",
      "source": "stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stacked_averaged_models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1fdfb0be04c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstacked_averaged_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstacked_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_averaged_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstacked_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_averaged_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmsle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_train_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stacked_averaged_models' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fc8845d7a65cc170ba9b2d88ea6c3321593b295"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}